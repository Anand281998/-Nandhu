{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset into a variable\n",
    "f = pd.read_csv('G:\\Data Sets\\germanCredit.data',delim_whitespace = True,header=None)\n",
    "f.columns=['Current_Account' , 'Duration_in_month', 'Credit_History' , 'Purpose' , 'Credit_Amount' , 'Saving_Account', 'Present_Employment' , 'Instalment_Rate' , 'Status', 'Debtors', 'Residence','Property', 'Age', 'Installment_Plan', 'Housing' , 'Credits_at_bank', 'Job', 'Maintenance' , 'Telephone', 'Foreign_workers', 'Credit_Card_Approval']\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature variable\n",
    "x=f.iloc[:,:20].values\n",
    "x[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the given data using LABEL ENCODER\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "x[:,0]= le.fit_transform(x[:,0])\n",
    "x[:,2]= le.fit_transform(x[:,2])\n",
    "x[:,3]= le.fit_transform(x[:,3])\n",
    "x[:,5]= le.fit_transform(x[:,5])\n",
    "x[:,6]= le.fit_transform(x[:,6])\n",
    "x[:,8]= le.fit_transform(x[:,8])\n",
    "x[:,9]= le.fit_transform(x[:,9])\n",
    "x[:,11]= le.fit_transform(x[:,11])\n",
    "x[:,13]= le.fit_transform(x[:,13])\n",
    "x[:,14]= le.fit_transform(x[:,14])\n",
    "x[:,16]= le.fit_transform(x[:,16])\n",
    "x[:,18]= le.fit_transform(x[:,18])\n",
    "x[:,19]= le.fit_transform(x[:,19])\n",
    "x[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data into feature and label variabels\n",
    "A = x\n",
    "B= pd.get_dummies(f.Credit_Card_Approval)\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test data\n",
    "x_train,x_test,y_train,y_test = train_test_split(A,B,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "lr_rate = 0.01\n",
    "epochs = 50\n",
    "batch_size =500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural parameters\n",
    "num_input = 20\n",
    "hid_layer1  = 100\n",
    "hid_layer2 = 50\n",
    "hid_layer3 = 25\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the place holders\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the values of weight(W) and biases(b)\n",
    "W = {'h1':tf.Variable(tf.random_normal([num_input,hid_layer1])),\n",
    "     'h2':tf.Variable(tf.random_normal([hid_layer1,hid_layer2])),\n",
    "     'h3':tf.Variable(tf.random_normal([hid_layer2,hid_layer3])),\n",
    "    'out':tf.Variable(tf.random_normal([hid_layer3,num_classes]))}\n",
    "\n",
    "B = {'h1':tf.Variable(tf.random_normal([hid_layer1])),\n",
    "     'h2':tf.Variable(tf.random_normal([hid_layer2])),\n",
    "     'h3':tf.Variable(tf.random_normal([hid_layer3])),\n",
    "    'out':tf.Variable(tf.random_normal([num_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the layers\n",
    "def neu_network(X):\n",
    "    l_1 = tf.add(tf.matmul(X,W['h1']),B['h1'])\n",
    "    L1 = tf.nn.relu(l_1)\n",
    "    \n",
    "    l_2 = tf.add(tf.matmul(l_1,W['h2']),B['h2'])\n",
    "    L2 = tf.nn.sigmoid(l_2)\n",
    "    \n",
    "    l_3 = tf.add(tf.matmul(l_2,W['h3']),B['h3'])\n",
    "    L3 = tf.nn.sigmoid(l_3)\n",
    "    \n",
    "    layer_out = tf.add(tf.matmul(l_3,W['out']),B['out'])\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function and calculating the result of the outputlayer\n",
    "y = neu_network(X)\n",
    "#y = tf.reduce_sum(y,axis= 1)\n",
    "y_out = tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the costfunction\n",
    "costfun = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y,labels=Y))\n",
    "#optimiser\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr_rate).minimize(costfun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinalTest Accuracy \n",
    "cor_pred = tf.equal(tf.argmax(y_out,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(cor_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing global variable initializer\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training the data\n",
    "for epoch in range (epochs):\n",
    "    sess.run(opt,feed_dict = {X:x_train,Y:y_train})\n",
    "    lf = sess.run(costfun,feed_dict = {X:x_train ,Y:y_train})\n",
    "    acc = sess.run(accuracy,feed_dict = {X:x_train ,Y : y_train})\n",
    "    print(\"Epoch:\",epoch  ,'----','cost:',lf,'----','----','Training Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final mean square error\n",
    "y_pred = sess.run(y,feed_dict={X:x_test})\n",
    "\n",
    "# Accuracy of the test data\n",
    "print(\"Test Accuracy:\",sess.run(accuracy,feed_dict = {X:x_test ,Y:y_test})*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the MEAN SQUARED ERROR\n",
    "mse = tf.reduce_mean(tf.square(y_pred - y_test))\n",
    "print('MSE: %4f',sess.run(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing global variable initializer\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training the data\n",
    "for epoch in range (epochs):\n",
    "    for i in range (len(x_train)):\n",
    "        start = i\n",
    "        end = i + batch_size\n",
    "        x_train = x_train[start:end]\n",
    "        y_train = y_train[start:end]\n",
    "        \n",
    "        sess.run(opt,feed_dict = {X:x_train,Y:y_train})\n",
    "        i += batch_size\n",
    "        lf = sess.run(costfun,feed_dict = {X:x_train ,Y:y_train})\n",
    "        acc = sess.run(accuracy,feed_dict = {X:x_train ,Y : y_train})\n",
    "        print(\"Epoch:\",epoch  ,'----','cost:',lf,'----','----','Training Accuracy:',acc)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
